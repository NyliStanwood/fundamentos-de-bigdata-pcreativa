# Dockerfile for training Spark MLlib model
FROM python:3.7-slim

# Install dependencies for SDKMAN
RUN apt-get update && \
    apt-get install -y curl zip unzip wget procps && \
    rm -rf /var/lib/apt/lists/*

# Install SDKMAN and Java 8
RUN curl -s "https://get.sdkman.io" | bash && \
    bash -c "source /root/.sdkman/bin/sdkman-init.sh && \
    sdk install java 8.0.302-open && \
    sdk default java 8.0.302-open"

# Set JAVA_HOME and JDK_HOME environment variables
ENV JAVA_HOME=/root/.sdkman/candidates/java/8.0.302-open
ENV JDK_HOME=/root/.sdkman/candidates/java/8.0.302-open
ENV PATH=$JAVA_HOME/bin:$PATH

# Install Spark 3.5.3
ENV SPARK_VERSION=3.5.3
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark
ENV PATH=$SPARK_HOME/bin:$PATH
ENV PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-*-src.zip

RUN wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    tar -xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} ${SPARK_HOME} && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

# Set working directory
WORKDIR /

# Copy requirements and install Python dependencies
COPY requirements_training.txt requirements.txt
RUN pip install --no-cache-dir -r requirements.txt


# Create models directory if it doesn't exist
RUN mkdir -p models

# Default command: train the model
CMD ["python3", "resources/train_spark_mllib_model.py", "."]
