## SPARK Flight Predictor

use the spark image
https://hub.docker.com/r/apache/spark/

to have java 8 configured

ENV JAVA_HOME
ENV JDK_HOME
ENV PATH=$JAVA_HOME/bin:$PATH

to have Spark 3.5.3 and configure the
ENV SPARK_HOME

copy the current repository files into the container.

to run the code

cd /practica_creativa/flight_prediction

spark-submit \
 --class es.upm.dit.ging.predictor.MakePrediction \
 --master local[*] \
 --packages org.mongodb.spark:mongo-spark-connector_2.12:10.4.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 \
 target/scala-2.12/flight_prediction_2.12-0.1.jar

docker-compose to start the container and
should share the /models that was created by this

```
services:
  {...}
  training:
    build:
      context: .
      dockerfile: Dockerfile.training
    container_name: ml-training
```

spark-submit connects to mongo service, make sure it can connect ej:

MONGO_HOST=${MONGO_HOST:-mongo}
MONGO_PORT=${MONGO_PORT:-27017}
MONGO_USER=${MONGO_USER:-}
MONGO_PASS=${MONGO_PASS:-}
//MONGO_INITDB_ROOT_USERNAME: root
//MONGO_INITDB_ROOT_PASSWORD: example
MONGO_AUTH_DB=${MONGO_AUTH_DB:-admin}

mongodb://${MONGO_HOST}:${MONGO_PORT}/${MONGO_DB}" ${MONGO_USER:+--username} ${MONGO_USER:+"${MONGO_USER}"} ${MONGO_PASS:+--password} ${MONGO_PASS:+"${MONGO_PASS}"}

docker-compose down
docker-compose up -d --build
docker-compose logs -f sparksubmit

Message Processing Flow in Your Spark Cluster

1. Message Arrival & Driver Processing (spark-submit container)
   When a Kafka message arrives on the flight-delay-ml-request topic:

The spark-submit container (driver process) is continuously listening via Kafka streaming
The driver receives the message and deserializes it (likely flight data for prediction)
The driver parses the data and creates a Spark Job - this is the logical unit of work 2. Driver Communicates with Master
The driver immediately connects to spark-master (via spark://spark-master:7077):

Driver sends a request to register the job/stage and requests executor slots
Master maintains a Resource Manager that tracks available executors on workers
Master responds with executor assignments from available workers (spark-worker-1 and spark-worker-2) 3. Task Scheduling & Distribution
The master breaks down the job into Tasks and creates a Execution Plan:

Logical Plan: How the data should flow (read from Kafka → extract features → apply ML model)
Physical Plan: Converts logical ops to RDD transformations
Stage Creation: Divides into stages based on shuffle boundaries
The master assigns tasks to specific executors on the workers 4. Worker Execution
The workers (spark-worker-1, spark-worker-2) receive the assigned tasks:

Each worker spawns Executors (JVM processes) that execute the tasks
Executors load the necessary code, Scala/Java dependencies (MongoDB connector, Kafka packages)
Each executor:
Deserializes the flight prediction data
Executes your MakePrediction class logic
Applies feature extraction transformations
Loads the pre-trained ML model from models (mounted volume)
Performs inference/prediction on the flight data 5. Data Shuffling (if applicable)
If your job has a reduceByKey or groupByKey operation:

Wide Transformation: Requires data movement between partitions
Workers shuffle data across the network via the Shuffle Service
Partition data is aggregated for the next stage 6. Result Aggregation
After all tasks complete:

Worker executors return results to the driver
Driver collects results in memory (or spills to disk if large)
The final RDD/DataFrame is created in driver memory 7. Output & MongoDB Storage
The driver (spark-submit) processes final results:

Connects to MongoDB (mongo container on port 27017)
Writes prediction results to agile_data_science database
Uses credentials: root:example with auth on admin DB
Data persists in MongoDB volume (mongo_data)

Architecture Summary in Your Config

┌─────────────────────────────────────────────┐
│ spark-submit (Driver) │
│ - Listens to Kafka topic │
│ - Creates jobs from messages │
│ - Schedules tasks on workers │
│ - Aggregates results │
│ - Writes to MongoDB │
└──────────────┬──────────────────────────────┘
│ (Master URL: spark://spark-master:7077)
│
┌──────▼──────────────┐
│ spark-master │
│ - Web UI: :8080 │
│ - RPC: :7077 │
│ - Resource Manager │
│ - Task Scheduler │
└──────┬──────────────┘
│
┌──────┴──────────────────────────┐
│ │
┌──────▼──────────┐ ┌─────────▼────────┐
│ spark-worker-1 │ │ spark-worker-2 │
│ Web UI: :8081 │ │ Web UI: :8082 │
│ Executors │ │ Executors │
│ - Run tasks │ │ - Run tasks │
│ - Load model │ │ - Load model │
│ - Compute │ │ - Compute │
└─────────────────┘ └──────────────────┘

Key Points in Your Configuration
Parallelism: Tasks are distributed across 2 workers, allowing simultaneous processing of multiple Kafka messages
Fault Tolerance: If a worker fails, the master reassigns tasks to the remaining worker
Model Loading: Your ML model is mounted on workers (/practica_creativa/models)
Network: All containers communicate via flight-network bridge network
Data Flow: Kafka → Driver → Workers → MongoDB
This setup enables distributed, scalable flight delay prediction with automatic task distribution and load balancing across your cluster.

Where Spark Jobs Are Created in Your Code
Phase 1: Data Parsing (Lines 46-76)

val df = spark
.readStream
.format("kafka")
.option("kafka.bootstrap.servers", "kafka:9092")
.option("subscribe", "flight-delay-ml-request")
.load()

val flightJsonDf = df.selectExpr("CAST(value AS STRING)")

val struct = new StructType()
.add("Origin", DataTypes.StringType)
// ... schema definition ...

val flightNestedDf = flightJsonDf.select(from_json($"value", struct).as("flight"))

ere's where data is parsed:

The driver reads Kafka messages as a streaming DataFrame
from_json() deserializes the JSON message into structured format
The schema defines how to interpret the raw Kafka payload

val flightFlattenedDf = flightNestedDf.selectExpr(...) // Flatten nested data
val predictionRequestsWithRouteMod = flightFlattenedDf.withColumn(...) // Add Route column
val predictionRequestsWithRoute = stringIndexerModel.map(n=>n.transform(...)) // String indexing
val vectorizedFeatures = vectorAssembler.setHandleInvalid("keep").transform(...) // Vectorize
val finalVectorizedFeatures = vectorizedFeatures.drop(...) // Clean up

These are lazy transformations - they create a Logical Execution Plan but don't execute yet. They define:

Select which columns to use
Apply ML feature transformations (string indexing)
Vectorize numeric features
Drop unnecessary columns

val predictions = rfc.transform(finalVectorizedFeatures)
.drop("Features_vec")

val finalPredictions = predictions.drop("indices").drop("values")...

Phase 4: Spark Job Creation (Lines 130-147)

val dataStreamWriter = finalPredictions
.writeStream
.format("mongodb")
.option("spark.mongodb.connection.uri", "mongodb://root:example@mongo:27017/agile_data_science?authSource=admin")
.option("spark.mongodb.database", "agile_data_science")
.option("checkpointLocation", "/tmp")
.option("spark.mongodb.collection", "flight_delay_ml_response")
.outputMode("append")

val query = dataStreamWriter.start() // <-- THIS TRIGGERS JOB CREATION

When .start() is called ← This is the critical moment:

Driver submits the logical plan to Master - All the transformations become actual stages
Master creates a Job - The complete workflow (parse → transform → predict → write to MongoDB)
Job is divided into Stages - Based on shuffle boundaries
Stages divided into Tasks - Each task assigned to a worker executor
Tasks execute in parallel on spark-worker-1 and spark-worker-2

Visual Summary

Kafka Message Arrives
↓
readStream (Line 46-52) ← READS from Kafka topic
↓
from_json() (Line 64) ← PARSES JSON to schema
↓
selectExpr(), withColumn() (Lines 77-119) ← BUILDS LOGICAL PLAN (not yet executed)
↓
rfc.transform() (Line 121) ← APPLIES ML MODEL (still lazy)
↓
.start() (Line 147) ← ⚡ TRIGGERS JOB CREATION & SUBMISSION ⚡
↓
Master receives Job → Creates Stages → Distributes Tasks to Workers
↓
Workers execute in parallel
↓
Results written to MongoDB (line 143-146)

In summary: The job creation happens at line 147 with .start() which triggers the execution of the entire streaming pipeline on your Spark cluster.
